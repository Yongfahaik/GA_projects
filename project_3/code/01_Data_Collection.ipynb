{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "496e4fef",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\" width=100>\n",
    "</div>\n",
    "\n",
    "# Project 3: Web APIs & NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ed622f",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "[Reddit](https://www.reddit.com) is a network of communities where people can dive into their interests, hobbies and passions. Subreddits are user-created channels where discussion on the topic of interest, hobby or passion are organized. From [Metrics For Reddit](https://frontpagemetrics.com/history), there are over 3.2 million subreddits as of December 2021, with hundreds of subreddits being created every day. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c50a026",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "As there are many different subreddits on Reddit, and since interests, hobbies and passions can be similar, there are always various subreddits that are similar to each other. Without a doubt, anyone who is new to writing and posting to Reddit can be confused as to which subreddit to post to. \n",
    "\n",
    "In this project, the aim is to assist the new Reddit user in the decision of which subreddit to make the post in, through the use of classification models based on the analysis of the posts of the subreddits.\n",
    "\n",
    "For the context of this project, the post is in the form of a scary experience, and the choices of the subreddits the new Reddit user has for making the post are [nosleep](https://www.reddit.com/r/nosleep/) and [paranormal](https://www.reddit.com/r/paranormal/), two subreddits that cater to scary personal experiences and paranormal experiences, thoughts and theories. \n",
    "\n",
    "To determine how successful the classification model is, the **accuracy** of the model as well as the **specificity** of the model will be the defining factors. Choosing accuracy is obvious as having the post going to the wrong subreddit is a bad idea, but choosing specificity as well is due to the fact thatt the `nosleep` subreddit has strict posting requirements, and thus it will be good to reduce the possibility of the post being wrongfully posted to the `nosleep` subreddit and having the post removed.\n",
    "\n",
    "At the end of this project, by determining the top root word features of each subreddit, we will better assist in the decision-making process of the user based on the given post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c46e19",
   "metadata": {},
   "source": [
    "## Part 1: Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89944aa5",
   "metadata": {},
   "source": [
    "### 1. Imports (All imported libraries are added here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f6e12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5e5b8b",
   "metadata": {},
   "source": [
    "### 2. Parameters for Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "351674f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url for the pushshift api\n",
    "url = \"https://api.pushshift.io/reddit/search/submission\"   \n",
    "\n",
    "# subreddits chosen\n",
    "subreddits = ['nosleep','paranormal']\n",
    "\n",
    "# Posts before 31 December 2021, 23:59:59, for the initial point of collection (time in Unix epoch time)\n",
    "last_time = 1640966399     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f38cb77",
   "metadata": {},
   "source": [
    "### 3. Function for Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17b5df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(sub, num_posts):\n",
    "    # Empty list for the data collection to output\n",
    "    posts = [] \n",
    "    \n",
    "     # initialize N for number of posts collected as count\n",
    "    N = 0\n",
    "    \n",
    "    # initialize the last variable\n",
    "    last = last_time\n",
    "    \n",
    "    # while loop to collect N posts\n",
    "    while N < num_posts:          \n",
    "        # parameters for the pushshift api: subbreddit, size of 100 posts each loop, before time\n",
    "        params = {\n",
    "            'subreddit': sub,   \n",
    "            'size': 100,\n",
    "            'before': last\n",
    "        }\n",
    "        \n",
    "        # api request and save to json\n",
    "        request = requests.get(url, params)\n",
    "        res = request.json()\n",
    "        \n",
    "        # add posts to list\n",
    "        posts.extend(res['data'])\n",
    "        \n",
    "        # Redefining parameters for while loop\n",
    "        last = int(res['data'][-1]['created_utc'])\n",
    "        \n",
    "        \n",
    "        # increase count by 100 for number of posts\n",
    "        N += 100\n",
    "    \n",
    "    return posts     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4786b42",
   "metadata": {},
   "source": [
    "### 4. Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6cb13e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# data collection for the first subreddit nosleep\n",
    "nosleep_posts = get_posts(subreddits[0], 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6000a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 42.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# data collection for the second subreddit paranormal\n",
    "paranormal_posts = get_posts(subreddits[1], 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9372a1",
   "metadata": {},
   "source": [
    "### 5. Output to DataFrame and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b42c8e29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# output to dataframe\n",
    "df_nosleep = pd.DataFrame(nosleep_posts)\n",
    "df_paranormal = pd.DataFrame(paranormal_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b0cf553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "df_nosleep.to_csv('../datasets/nosleep_posts.csv', index=False)\n",
    "df_paranormal.to_csv('../datasets/paranormal_posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fb490c",
   "metadata": {},
   "source": [
    "### 6. Progress thus far\n",
    "At this point in time, we have completed the webscraping of the two subreddits and compiled them into csvs in order to prep the data for analysis. \n",
    "\n",
    "We will continue next in Part 2: [EDA_and_Data_Cleaning](./02_EDA_and_Data_Cleaning.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
